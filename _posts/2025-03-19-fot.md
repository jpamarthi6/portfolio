---
layout: post
title: 'Designing Task-Centered HMI Strategies for Human-AI Collaboration'
# permalink: /FOT/
---
### Problem:

As Level 4 autonomous trucks become a reality, how do we decide what tasks still need a human—and how do we communicate those through interface design? The trucking industry is advancing rapidly, but there's limited guidance on how to split responsibilities between humans and automation—or how to reflect that in HMI design. Our goal: identify key tasks that still need human input and design HMI strategies that foster trust and clarity.

---
{: data-content="hr with text"}

### Solution:
- Identified gaps in multimodal HMIs (most focus on visual-only systems)[^2].
- Proposed interface features to improve explainability and situational awareness.

---
{: data-content="hr with text"}

### Study:

Over 3 months, I led a research project to map automation gaps and design better HMIs for safe, explainable, and collaborative human-machine interaction in trucking.

What I Did:
- Defined Research Focus  
  Mapped key questions: What trucking tasks need humans? What HMI modes are underused?

- Mapped Research Gaps  
  Reviewed 100+ studies and created a matrix of automation level × HMI modality × task type

- Analyzed CDL Tasks  
  Used truck driver manuals to identify real-world tasks across driving, inspection, and planning

- Applied Coactive Design  
  Labeled tasks as fully automatable or requiring human input, and listed constraints (e.g., trust, legal)

- Developed HMI Guidelines  
  Proposed features like context-aware explanations (e.g., why a reroute happened) to boost transparency

- Mentored Research Team  
  Taught task analysis and synthesis skills while guiding report and presentation creation

- Delivered Multi-Audience Outputs  
  Created a written report, annotated bibliography, stakeholder-ready slides, and research matrices

---
{: data-content="hr with text"}

### Publication: 

Coming soon: internal simulation-based HMI prototypes and academic write-up in progress.


