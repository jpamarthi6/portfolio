---
layout: post
title: 'Evaluating Usability for Remote AV Operators'
---
#### UX Research Internship at TORC Robotics
---
{: data-content="hr with text"}
### Overview:

At Motional, I designed and ran a usability study to evaluate how well remote operators could understand and respond to AV status updates. The study measured reaction time, decision accuracy, and mental workload using a custom-built visual task—helping assess how interface design supports real-time remote supervision.

---
{: data-content="hr with text"}
### Problem:

As AVs scale, remote operators play a key safety role. But unclear alerts, visual overload, or interface delays can slow operator decisions. We needed to understand:
- Can current interfaces support rapid, accurate decision-making under pressure?
- What cognitive bottlenecks affect operator performance?

---
{: data-content="hr with text"}
### Users:

**Remote support operators** responsible for supervising AV behavior and intervening when needed. They must process incoming visual cues and act quickly—with little room for error.

---
{: data-content="hr with text"}
### My Role:

As a UX Research Intern, I:
- **Designed a controlled usability task** simulating AV alerts
- **Collected behavioral and feedback data** from remote operators
- **Analyzed performance** using Python and R to identify UI pain points

---
{: data-content="hr with text"}
### Constraints:

- No access to full production interface (due to IP restrictions)
- Remote deployment required simulation without disclosing real UI
- Cognitive realism needed without overwhelming users

---
{: data-content="hr with text"}
### Process & What I Did:

1. **Built a Visual Handoff Task in PsychoPy** <br>
    - Simulated time-sensitive AV alerts using visual stimuli
    - Measured response time and accuracy as users processed system cues
    - Designed task logic based on real-world handoff scenarios

    Goal: Evaluate how well operators detect and respond to AV transitions or requests

2. **Ran Mixed-Methods Usability Study** <br>
    - Recruited and tested remote operators in a virtual setup
    - Collected:
        - **Behavioral metrics:** reaction time, accuracy
        - **Subjective feedback:** perceived difficulty, clarity
    - Identified:
        - Delays in response due to unclear visual hierarchy
        - Information overload in high-alert scenarios
    
    Lesson: Operators perform better when alerts are visually prioritized and contextualized—not just fast.

3. **Analyzed Results Using Python & R** <br>
    - Cleaned and merged task logs, survey responses, and timestamps
    - Used ANOVA and ggplot2 to compare performance across alert types
    - Created visual dashboards showing slowdowns and confusion points

    Finding: Certain alert layouts caused statistically significant delays in operator reaction time

---
{: data-content="hr with text"}

### Key Takeaways:
- Clear, prioritized alerts lead to faster and safer decisions
- Under time pressure, **cognitive load becomes the hidden UX problem**
- Simulated tasks can reveal real-world design risks—even without full UI access

---
{: data-content="hr with text"}

### Impact:

- Study results informed UI adjustments for remote AV monitoring
- Helped Motional benchmark operator readiness and task load
- Reinforced the importance of **cognitive UX testing** in safety-critical systems

---
{: data-content="hr with text"}

